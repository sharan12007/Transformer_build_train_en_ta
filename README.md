
## ğŸ§  **English â†’ Tamil Neural Machine Translation**

### *Transformer-based Translation Model with Advanced Training Techniques*

[Transformer Architecture](https://arxiv.org/abs/1706.03762)

> A custom-built Transformer architecture trained from scratch for English â†’ Tamil translation using PyTorch.

---

### ğŸš€ **Overview**

This project implements a complete **Transformer-based Neural Machine Translation (NMT)** system trained **from scratch** on a large-scale Englishâ€“Tamil parallel corpus.
The architecture, training loop, and tokenization are all designed manually (no pre-trained models used).

---

### âš™ï¸ **Key Features and Advanced Techniques**

| Category                       | Technique                           | Description                                                                                                                           |
| ------------------------------ | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| **Architecture**               | âœ¨ Custom Transformer                | Encoderâ€“Decoder Transformer (4 layers each, 8 heads, 512-dim) implemented manually using PyTorch.                                     |
| **Tokenization**               | ğŸ§© Byte-Level BPE                   | Trained custom BPE tokenizers for both English & Tamil using Hugging Face `tokenizers` â€” handles complex Tamil diacritics & subwords. |
| **Data Handling**              | ğŸ“¦ Hugging Face Datasets            | Uses `Hemanth-thunder/en_ta` (200k+ sentence pairs) directly from Hugging Face.                                                       |
| **Training Scheduler**         | ğŸ§  Warmup + Cosine Annealing        | Combines learning rate warmup (stabilizes early training) and cosine annealing decay (for long-term convergence).                     |
| **Loss Function**              | ğŸ¯ Label Smoothing + Ignore Padding | Stabilizes gradients and prevents overconfidence in sequence prediction.                                                              |
| **Gradient Stability**         | ğŸ§® Gradient Clipping (1.0)          | Prevents exploding gradients during long-sequence training.                                                                           |
| **Model Saving**               | ğŸ’¾ Auto Checkpointing               | Saves checkpoints every epoch to Google Drive or local system.                                                                        |
| **Decoding**                   | ğŸŒŸ Beam Search (optional)           | Significantly improves translation quality and reduces repetition.                                                                    |
| **Mixed Precision (Optional)** | âš¡ AMP Integration                   | Enables faster training with reduced GPU memory (optional).                                                                           |
| **Colab-Ready**                | â˜ï¸ Google Drive Auto-Backup         | Automatically zips and downloads model + tokenizer after training.                                                                    |

---

### ğŸ“Š **Training Summary**

| Parameter       | Value                                                                          |
| --------------- | ------------------------------------------------------------------------------ |
| Model Type      | Transformer Encoderâ€“Decoder                                                    |
| Embedding Dim   | 512                                                                            |
| FFN Hidden Dim  | 2048                                                                           |
| Attention Heads | 8                                                                              |
| Encoder Layers  | 4                                                                              |
| Decoder Layers  | 4                                                                              |
| Dropout         | 0.2                                                                            |
| Optimizer       | AdamW                                                                          |
| Learning Rate   | 3e-5 (Warmup + Cosine decay)                                                   |
| Label Smoothing | 0.1                                                                            |
| Batch Size      | 32                                                                             |
| Dataset         | [Hemanth-thunder/en_ta](https://huggingface.co/datasets/Hemanth-thunder/en_ta) |
| Training Time   | ~5 hours (10 epochs on Colab T4 GPU)                                           |

---

### ğŸ“¦ **Repository Structure**

```
Transformer_mod/
â”œâ”€â”€ model.py                # Transformer architecture (encoder, decoder, attention)
â”œâ”€â”€ train.py                # Training pipeline (scheduler, loss, checkpointing)
â”œâ”€â”€ example.py              # Evaluation & inference script
â”œâ”€â”€ bpe_en_tokenizer.json   # Trained English BPE tokenizer
â”œâ”€â”€ bpe_ta_tokenizer.json   # Trained Tamil BPE tokenizer
â”œâ”€â”€ checkpoint_epoch_*.pth  # Model checkpoints
â””â”€â”€ README.md               # Project documentation
```

---

### ğŸ”§ **Setup and Installation**

#### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/sharaneshwar182007/Transformer_mod.git
cd Transformer_mod
```

#### 2ï¸âƒ£ Install dependencies

```bash
pip install torch datasets tokenizers tqdm
```

#### 3ï¸âƒ£ (Optional) Run in Colab with Drive backup

Mount Google Drive:

```python
from google.colab import drive
drive.mount('/content/drive')
```

---

### ğŸ‹ï¸â€â™‚ï¸ **Training the Model**

```bash
python train.py
```

This will:

* Load the `Hemanth-thunder/en_ta` dataset
* Train the Transformer for 10 epochs
* Save checkpoints (`/content/drive/MyDrive/EN_TA_Checkpoints/`)
* Automatically download the trained model ZIP file after training

---

### ğŸ§ª **Evaluating and Translating**

After training (or using an existing checkpoint), run:

```bash
python example.py
```

Then enter sentences interactively:

```
Enter the input in English: how are you
Translation in Tamil: à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯
```

You can exit with `exit`.

---

### ğŸ’¡ **Advanced Techniques Explained**

#### ğŸ§  1. Warmup + Cosine Annealing

Smoothly increases LR for first few epochs â†’ avoids unstable gradients.
Then gradually decreases using cosine decay â†’ better long-term convergence.

#### ğŸ§© 2. Byte-Level BPE Tokenization

Tamil has complex script combinations.
Byte-level BPE captures rare and compound characters without splitting words incorrectly.

#### âš™ï¸ 3. Label Smoothing

Makes training more robust:

```python
nn.CrossEntropyLoss(ignore_index=1, label_smoothing=0.1)
```

#### ğŸ§® 4. Gradient Clipping

Clips gradients to prevent instability:

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

#### ğŸŒˆ 5. Beam Search Decoding

Improves translation quality by exploring multiple candidate sequences instead of greedy decoding.

---

### ğŸ“ˆ **Expected Training Behavior**

| Epoch | Loss      | Quality                     |
| ----- | --------- | --------------------------- |
| 1â€“5   | 4.5 â†’ 3.8 | Random Tamil-like tokens    |
| 6â€“10  | 3.8 â†’ 3.2 | Word fragments appear       |
| 11â€“15 | 3.2 â†’ 2.8 | Coherent short translations |
| 16â€“20 | 2.8 â†’ 2.5 | Fluent, meaningful Tamil    |

---

### ğŸ§© **Future Enhancements**

* âœ… Integrate **BLEU / SacreBLEU** for quantitative evaluation
* âœ… Add **beam search decoding** (top-k or nucleus sampling)
* ğŸ”œ Support **bi-directional translation** (Tamil â†’ English)
* ğŸ”œ Use **mixed-precision** for faster GPU training
* ğŸ”œ Integrate with **Hugging Face Transformers** for deployment

---

### ğŸ‘¨â€ğŸ’» **Author**

**G. Sharan Eshwar**
ğŸ“§ [sharaneshwar182007@gmail.com](mailto:sharaneshwar182007@gmail.com)
ğŸš€ Hobby Researcher in DeepNeural Network,Geospatial AI & Transformer-based Language Models

---

### â­ **If you find this project useful**

Please **star ğŸŒŸ the repo** on GitHub â€” it helps others discover it.

---
